{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r-cII9C3biL3",
        "outputId": "0b892085-4f78-4d1b-d2f4-2a9caf51ffa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NmBxVVrwFDh"
      },
      "outputs": [],
      "source": [
        "import zipfile,os "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1US84ItwUvo"
      },
      "outputs": [],
      "source": [
        "base_dir  = '/content/drive/MyDrive/dataset-5000'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvK9e3rKwpQy",
        "outputId": "9a46858d-a76b-4735-85c7-a9bcccdbea6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nonhijab', 'hijab']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "os.listdir('/content/drive/MyDrive/dataset-5000/train')\n",
        "os.listdir('/content/drive/MyDrive/dataset-5000/val')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#torch"
      ],
      "metadata": {
        "id": "1mQhXyTlzfjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model old"
      ],
      "metadata": {
        "id": "30dlC8Ag3gwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomVGG(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(CustomVGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3,padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256 ,256,kernel_size = 3,padding = 1 ),\n",
        "           \n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7*7*512))\n",
        "        self.classifier =nn.Sequential(\n",
        "            \n",
        "           torch.nn.Linear(in_features=(7*7*512), out_features=num_classes,bias=True)\n",
        "           \n",
        "           \n",
        "           \n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        \n",
        "        x=self.features(x)\n",
        "        \n",
        "        x=self.avgpool(x)\n",
        "        \n",
        "        x=torch.flatten(x,start_dim = 1)\n",
        "        \n",
        "        x=self.classifier(x)\n",
        "        \n",
        "        \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-70gcHGuzdo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CustomVGG(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        \n",
        "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            \n",
        "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            \n",
        "            nn.Flatten(),\n",
        "            nn.Linear(82944,1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ],
      "metadata": {
        "id": "3IYaNx_pXlNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomVGG(nn.Module):\n",
        "\n",
        "    # Defining the Constructor\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(CustomVGG, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # We in the end apply max pooling with a kernel size of 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
        "        self.drop = nn.Dropout2d(p=0.2)\n",
        "        \n",
        "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
        "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
        "        \n",
        "        # We need to flatten these in order to feed them to a fully-connected layer\n",
        "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # In the forward function, pass the data through the layers we defined in the init function\n",
        "        \n",
        "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
        "        x = F.relu(self.pool(self.conv1(x))) \n",
        "        \n",
        "        # Use a ReLU activation function after layer 2\n",
        "        x = F.relu(self.pool(self.conv2(x)))  \n",
        "        \n",
        "        # Select some features to drop to prevent overfitting (only drop during training)\n",
        "        x = F.dropout(self.drop(x), training=self.training)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(-1, 32 * 32 * 24)\n",
        "        # Feed to fully-connected layer to predict class\n",
        "        x = self.fc(x)\n",
        "        # Return class probabilities via a log_softmax function \n",
        "        return torch.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "nNChjmzeheSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##last model"
      ],
      "metadata": {
        "id": "KBVRAZm1OYAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # onvolutional layers (3,16,32)\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 64 * 6 * 6, out_features=500)\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
        "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = F.relu(self.conv1(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = F.relu(self.conv2(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = F.relu(self.conv3(X))\n",
        "        X = F.max_pool2d(X, 2)\n",
        "\n",
        "        X = X.view(X.shape[0], -1)\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        X = self.fc3(X)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "A5ofJJ6-_cr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data"
      ],
      "metadata": {
        "id": "9pw6u0CS3jhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
      ],
      "metadata": {
        "id": "0jdBc2-07Lkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# torch.cuda.empty_cache()\n",
        "# import gc\n",
        "# try:\n",
        "#    del model\n",
        "# except:\n",
        "#   pass\n",
        "# gc.collect()"
      ],
      "metadata": {
        "id": "TlRIjmD14H8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomVGG().to(device)"
      ],
      "metadata": {
        "id": "N5BkBaiXX8B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load train and test datasets\n",
        "train_dataset = datasets.ImageFolder('/content/drive/MyDrive/dataset-5000/train', transform=transform)\n",
        "test_dataset = datasets.ImageFolder('/content/drive/MyDrive/dataset-5000/val', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "iY1DPTR40Pr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "N6gapRDm3lfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "def train_test():\n",
        "    # Loop over your data for multiple epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images_batch, labels_batch )in enumerate(train_loader):\n",
        "            images_batch, labels_batch = images_batch.cuda(), labels_batch.cuda() \n",
        "            # Zero out gradients from previous iteration\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass through the network\n",
        "            output_batch = model(images_batch)\n",
        "            \n",
        "            # Compute loss between predicted and true labels\n",
        "            loss = criterion(output_batch , labels_batch)\n",
        "            \n",
        "            # Backpropagate gradients through the network\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update weights using optimizer step\n",
        "            optimizer.step()\n",
        "            # Print the loss for this step\n",
        "            #print('Loss: {:.6f}'.format(loss.item()))\n",
        "            # Compute accuracy\n",
        "            _, predicted = torch.max(output_batch.data, 1)\n",
        "            correct = (predicted == labels_batch).sum().item()\n",
        "            total = labels_batch.size(0)\n",
        "            accuracy = correct / total\n",
        "\n",
        "            # Print loss and accuracy\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item(), accuracy*100))\n",
        "\n",
        "    # Evaluate your model on test data after training is complete\n",
        "    torch.save(model.state_dict(), 'hijab_model_torch.pth')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        \n",
        "        for images_batch , labels_batch in test_loader:\n",
        "            images_batch, labels_batch = images_batch.cuda(), labels_batch.cuda() \n",
        "            output=model(images_batch)\n",
        "            \n",
        "            _,predicted=torch.max(output.data,dim=-1)\n",
        "            \n",
        "            correct+= (predicted == labels_batch).sum().item()\n",
        "\n",
        "        accuracy=(correct/len(test_loader.dataset))*100\n",
        "        print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "id": "BKnwgQR4z35H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_test() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n440JSK6WFri",
        "outputId": "9f119bb2-8bc1-4c81-b20e-c17700fc1ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15], Step [1/126], Loss: 0.6907, Accuracy: 54.69%\n",
            "Epoch [1/15], Step [2/126], Loss: 0.6385, Accuracy: 64.06%\n",
            "Epoch [1/15], Step [3/126], Loss: 0.8245, Accuracy: 54.69%\n",
            "Epoch [1/15], Step [4/126], Loss: 0.6847, Accuracy: 53.12%\n",
            "Epoch [1/15], Step [5/126], Loss: 0.6758, Accuracy: 50.00%\n",
            "Epoch [1/15], Step [6/126], Loss: 0.6801, Accuracy: 59.38%\n",
            "Epoch [1/15], Step [7/126], Loss: 0.6728, Accuracy: 76.56%\n",
            "Epoch [1/15], Step [8/126], Loss: 0.6749, Accuracy: 62.50%\n",
            "Epoch [1/15], Step [9/126], Loss: 0.6618, Accuracy: 59.38%\n",
            "Epoch [1/15], Step [10/126], Loss: 0.6689, Accuracy: 53.12%\n",
            "Epoch [1/15], Step [11/126], Loss: 0.6460, Accuracy: 59.38%\n",
            "Epoch [1/15], Step [12/126], Loss: 0.6025, Accuracy: 68.75%\n",
            "Epoch [1/15], Step [13/126], Loss: 0.5839, Accuracy: 71.88%\n",
            "Epoch [1/15], Step [14/126], Loss: 0.5967, Accuracy: 68.75%\n",
            "Epoch [1/15], Step [15/126], Loss: 0.5657, Accuracy: 75.00%\n",
            "Epoch [1/15], Step [16/126], Loss: 0.5385, Accuracy: 81.25%\n",
            "Epoch [1/15], Step [17/126], Loss: 0.4973, Accuracy: 82.81%\n",
            "Epoch [1/15], Step [18/126], Loss: 0.4685, Accuracy: 78.12%\n",
            "Epoch [1/15], Step [19/126], Loss: 0.4937, Accuracy: 78.12%\n",
            "Epoch [1/15], Step [20/126], Loss: 0.5397, Accuracy: 79.69%\n",
            "Epoch [1/15], Step [21/126], Loss: 0.5843, Accuracy: 73.44%\n",
            "Epoch [1/15], Step [22/126], Loss: 0.5973, Accuracy: 81.25%\n",
            "Epoch [1/15], Step [23/126], Loss: 0.4126, Accuracy: 82.81%\n",
            "Epoch [1/15], Step [24/126], Loss: 0.6337, Accuracy: 75.00%\n",
            "Epoch [1/15], Step [25/126], Loss: 0.3173, Accuracy: 87.50%\n",
            "Epoch [1/15], Step [26/126], Loss: 0.4802, Accuracy: 82.81%\n",
            "Epoch [1/15], Step [27/126], Loss: 0.3485, Accuracy: 85.94%\n",
            "Epoch [1/15], Step [28/126], Loss: 0.3785, Accuracy: 81.25%\n",
            "Epoch [1/15], Step [29/126], Loss: 0.4005, Accuracy: 82.81%\n",
            "Epoch [1/15], Step [30/126], Loss: 0.4356, Accuracy: 76.56%\n",
            "Epoch [1/15], Step [31/126], Loss: 0.3581, Accuracy: 85.94%\n",
            "Epoch [1/15], Step [32/126], Loss: 0.3591, Accuracy: 84.38%\n",
            "Epoch [1/15], Step [33/126], Loss: 0.3623, Accuracy: 90.62%\n",
            "Epoch [1/15], Step [34/126], Loss: 0.3592, Accuracy: 81.25%\n",
            "Epoch [1/15], Step [35/126], Loss: 0.3503, Accuracy: 84.38%\n",
            "Epoch [1/15], Step [36/126], Loss: 0.3131, Accuracy: 82.81%\n",
            "Epoch [1/15], Step [37/126], Loss: 0.4170, Accuracy: 79.69%\n",
            "Epoch [1/15], Step [38/126], Loss: 0.2431, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [39/126], Loss: 0.2077, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [40/126], Loss: 0.3088, Accuracy: 85.94%\n",
            "Epoch [1/15], Step [41/126], Loss: 0.2011, Accuracy: 89.06%\n",
            "Epoch [1/15], Step [42/126], Loss: 0.3910, Accuracy: 76.56%\n",
            "Epoch [1/15], Step [43/126], Loss: 0.2724, Accuracy: 90.62%\n",
            "Epoch [1/15], Step [44/126], Loss: 0.3527, Accuracy: 87.50%\n",
            "Epoch [1/15], Step [45/126], Loss: 0.2663, Accuracy: 89.06%\n",
            "Epoch [1/15], Step [46/126], Loss: 0.4356, Accuracy: 81.25%\n",
            "Epoch [1/15], Step [47/126], Loss: 0.4253, Accuracy: 84.38%\n",
            "Epoch [1/15], Step [48/126], Loss: 0.2452, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [49/126], Loss: 0.1531, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [50/126], Loss: 0.2571, Accuracy: 87.50%\n",
            "Epoch [1/15], Step [51/126], Loss: 0.2422, Accuracy: 90.62%\n",
            "Epoch [1/15], Step [52/126], Loss: 0.2328, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [53/126], Loss: 0.1217, Accuracy: 100.00%\n",
            "Epoch [1/15], Step [54/126], Loss: 0.2597, Accuracy: 87.50%\n",
            "Epoch [1/15], Step [55/126], Loss: 0.2508, Accuracy: 89.06%\n",
            "Epoch [1/15], Step [56/126], Loss: 0.1375, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [57/126], Loss: 0.4735, Accuracy: 81.25%\n",
            "Epoch [1/15], Step [58/126], Loss: 0.1455, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [59/126], Loss: 0.2731, Accuracy: 87.50%\n",
            "Epoch [1/15], Step [60/126], Loss: 0.1875, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [61/126], Loss: 0.2724, Accuracy: 89.06%\n",
            "Epoch [1/15], Step [62/126], Loss: 0.3052, Accuracy: 89.06%\n",
            "Epoch [1/15], Step [63/126], Loss: 0.1698, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [64/126], Loss: 0.2647, Accuracy: 85.94%\n",
            "Epoch [1/15], Step [65/126], Loss: 0.1324, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [66/126], Loss: 0.1512, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [67/126], Loss: 0.0883, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [68/126], Loss: 0.1715, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [69/126], Loss: 0.2237, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [70/126], Loss: 0.1935, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [71/126], Loss: 0.1297, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [72/126], Loss: 0.2223, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [73/126], Loss: 0.2263, Accuracy: 90.62%\n",
            "Epoch [1/15], Step [74/126], Loss: 0.1073, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [75/126], Loss: 0.2652, Accuracy: 87.50%\n",
            "Epoch [1/15], Step [76/126], Loss: 0.3272, Accuracy: 85.94%\n",
            "Epoch [1/15], Step [77/126], Loss: 0.1335, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [78/126], Loss: 0.0978, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [79/126], Loss: 0.1322, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [80/126], Loss: 0.0743, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [81/126], Loss: 0.1708, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [82/126], Loss: 0.2014, Accuracy: 89.06%\n",
            "Epoch [1/15], Step [83/126], Loss: 0.0841, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [84/126], Loss: 0.2376, Accuracy: 90.62%\n",
            "Epoch [1/15], Step [85/126], Loss: 0.1538, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [86/126], Loss: 0.2356, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [87/126], Loss: 0.2557, Accuracy: 89.06%\n",
            "Epoch [1/15], Step [88/126], Loss: 0.1717, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [89/126], Loss: 0.1291, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [90/126], Loss: 0.1977, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [91/126], Loss: 0.2079, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [92/126], Loss: 0.1433, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [93/126], Loss: 0.1162, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [94/126], Loss: 0.1143, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [95/126], Loss: 0.2241, Accuracy: 90.62%\n",
            "Epoch [1/15], Step [96/126], Loss: 0.1545, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [97/126], Loss: 0.0733, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [98/126], Loss: 0.1316, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [99/126], Loss: 0.2544, Accuracy: 89.06%\n",
            "Epoch [1/15], Step [100/126], Loss: 0.0838, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [101/126], Loss: 0.1313, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [102/126], Loss: 0.0458, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [103/126], Loss: 0.2971, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [104/126], Loss: 0.1239, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [105/126], Loss: 0.1606, Accuracy: 90.62%\n",
            "Epoch [1/15], Step [106/126], Loss: 0.1223, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [107/126], Loss: 0.0874, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [108/126], Loss: 0.2754, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [109/126], Loss: 0.1791, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [110/126], Loss: 0.0879, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [111/126], Loss: 0.1236, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [112/126], Loss: 0.1047, Accuracy: 95.31%\n",
            "Epoch [1/15], Step [113/126], Loss: 0.0722, Accuracy: 100.00%\n",
            "Epoch [1/15], Step [114/126], Loss: 0.0917, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [115/126], Loss: 0.0928, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [116/126], Loss: 0.0561, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [117/126], Loss: 0.1229, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [118/126], Loss: 0.1046, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [119/126], Loss: 0.0302, Accuracy: 98.44%\n",
            "Epoch [1/15], Step [120/126], Loss: 0.1594, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [121/126], Loss: 0.0588, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [122/126], Loss: 0.0390, Accuracy: 100.00%\n",
            "Epoch [1/15], Step [123/126], Loss: 0.0867, Accuracy: 96.88%\n",
            "Epoch [1/15], Step [124/126], Loss: 0.1807, Accuracy: 92.19%\n",
            "Epoch [1/15], Step [125/126], Loss: 0.3609, Accuracy: 93.75%\n",
            "Epoch [1/15], Step [126/126], Loss: 0.0605, Accuracy: 96.67%\n",
            "Epoch [2/15], Step [1/126], Loss: 0.0215, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [2/126], Loss: 0.0895, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [3/126], Loss: 0.0404, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [4/126], Loss: 0.0367, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [5/126], Loss: 0.1412, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [6/126], Loss: 0.1575, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [7/126], Loss: 0.1210, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [8/126], Loss: 0.0501, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [9/126], Loss: 0.0969, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [10/126], Loss: 0.0733, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [11/126], Loss: 0.0575, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [12/126], Loss: 0.0373, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [13/126], Loss: 0.0795, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [14/126], Loss: 0.0282, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [15/126], Loss: 0.0634, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [16/126], Loss: 0.0771, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [17/126], Loss: 0.0657, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [18/126], Loss: 0.0664, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [19/126], Loss: 0.1609, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [20/126], Loss: 0.2553, Accuracy: 90.62%\n",
            "Epoch [2/15], Step [21/126], Loss: 0.1286, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [22/126], Loss: 0.0966, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [23/126], Loss: 0.1465, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [24/126], Loss: 0.0463, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [25/126], Loss: 0.0860, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [26/126], Loss: 0.1583, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [27/126], Loss: 0.1596, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [28/126], Loss: 0.1002, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [29/126], Loss: 0.0666, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [30/126], Loss: 0.1772, Accuracy: 92.19%\n",
            "Epoch [2/15], Step [31/126], Loss: 0.0702, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [32/126], Loss: 0.0375, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [33/126], Loss: 0.0762, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [34/126], Loss: 0.1743, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [35/126], Loss: 0.0834, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [36/126], Loss: 0.0535, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [37/126], Loss: 0.0876, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [38/126], Loss: 0.0257, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [39/126], Loss: 0.1500, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [40/126], Loss: 0.0975, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [41/126], Loss: 0.0572, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [42/126], Loss: 0.0222, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [43/126], Loss: 0.0802, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [44/126], Loss: 0.1476, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [45/126], Loss: 0.0622, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [46/126], Loss: 0.0548, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [47/126], Loss: 0.0818, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [48/126], Loss: 0.0637, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [49/126], Loss: 0.0726, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [50/126], Loss: 0.0952, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [51/126], Loss: 0.0305, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [52/126], Loss: 0.0855, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [53/126], Loss: 0.0764, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [54/126], Loss: 0.0207, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [55/126], Loss: 0.0539, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [56/126], Loss: 0.0750, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [57/126], Loss: 0.0382, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [58/126], Loss: 0.0968, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [59/126], Loss: 0.1067, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [60/126], Loss: 0.0795, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [61/126], Loss: 0.0078, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [62/126], Loss: 0.0760, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [63/126], Loss: 0.0384, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [64/126], Loss: 0.0965, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [65/126], Loss: 0.0463, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [66/126], Loss: 0.0915, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [67/126], Loss: 0.0723, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [68/126], Loss: 0.0420, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [69/126], Loss: 0.2494, Accuracy: 87.50%\n",
            "Epoch [2/15], Step [70/126], Loss: 0.0678, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [71/126], Loss: 0.0456, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [72/126], Loss: 0.0941, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [73/126], Loss: 0.0294, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [74/126], Loss: 0.0866, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [75/126], Loss: 0.0498, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [76/126], Loss: 0.0522, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [77/126], Loss: 0.0366, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [78/126], Loss: 0.1013, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [79/126], Loss: 0.1083, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [80/126], Loss: 0.0630, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [81/126], Loss: 0.0750, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [82/126], Loss: 0.0310, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [83/126], Loss: 0.0240, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [84/126], Loss: 0.0396, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [85/126], Loss: 0.0316, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [86/126], Loss: 0.0529, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [87/126], Loss: 0.0972, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [88/126], Loss: 0.0309, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [89/126], Loss: 0.0174, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [90/126], Loss: 0.0704, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [91/126], Loss: 0.1523, Accuracy: 93.75%\n",
            "Epoch [2/15], Step [92/126], Loss: 0.0360, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [93/126], Loss: 0.0114, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [94/126], Loss: 0.0581, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [95/126], Loss: 0.0213, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [96/126], Loss: 0.0597, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [97/126], Loss: 0.0470, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [98/126], Loss: 0.0674, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [99/126], Loss: 0.0622, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [100/126], Loss: 0.0439, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [101/126], Loss: 0.0410, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [102/126], Loss: 0.1249, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [103/126], Loss: 0.0164, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [104/126], Loss: 0.0287, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [105/126], Loss: 0.0187, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [106/126], Loss: 0.0384, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [107/126], Loss: 0.0341, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [108/126], Loss: 0.0246, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [109/126], Loss: 0.0678, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [110/126], Loss: 0.0905, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [111/126], Loss: 0.0457, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [112/126], Loss: 0.0588, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [113/126], Loss: 0.0476, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [114/126], Loss: 0.0151, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [115/126], Loss: 0.1563, Accuracy: 92.19%\n",
            "Epoch [2/15], Step [116/126], Loss: 0.0701, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [117/126], Loss: 0.1484, Accuracy: 95.31%\n",
            "Epoch [2/15], Step [118/126], Loss: 0.0228, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [119/126], Loss: 0.0454, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [120/126], Loss: 0.0530, Accuracy: 98.44%\n",
            "Epoch [2/15], Step [121/126], Loss: 0.0654, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [122/126], Loss: 0.0210, Accuracy: 100.00%\n",
            "Epoch [2/15], Step [123/126], Loss: 0.0913, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [124/126], Loss: 0.0575, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [125/126], Loss: 0.0464, Accuracy: 96.88%\n",
            "Epoch [2/15], Step [126/126], Loss: 0.1167, Accuracy: 96.67%\n",
            "Epoch [3/15], Step [1/126], Loss: 0.0460, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [2/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [3/126], Loss: 0.0071, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [4/126], Loss: 0.0226, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [5/126], Loss: 0.0555, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [6/126], Loss: 0.0086, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [7/126], Loss: 0.0109, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [8/126], Loss: 0.0380, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [9/126], Loss: 0.0187, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [10/126], Loss: 0.0305, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [11/126], Loss: 0.0239, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [12/126], Loss: 0.0394, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [13/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [14/126], Loss: 0.0843, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [15/126], Loss: 0.0125, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [16/126], Loss: 0.0608, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [17/126], Loss: 0.1055, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [18/126], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [19/126], Loss: 0.0955, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [20/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [21/126], Loss: 0.1650, Accuracy: 95.31%\n",
            "Epoch [3/15], Step [22/126], Loss: 0.0131, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [23/126], Loss: 0.0276, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [24/126], Loss: 0.0330, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [25/126], Loss: 0.1656, Accuracy: 95.31%\n",
            "Epoch [3/15], Step [26/126], Loss: 0.0248, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [27/126], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [28/126], Loss: 0.0373, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [29/126], Loss: 0.0899, Accuracy: 95.31%\n",
            "Epoch [3/15], Step [30/126], Loss: 0.0421, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [31/126], Loss: 0.0855, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [32/126], Loss: 0.0587, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [33/126], Loss: 0.0118, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [34/126], Loss: 0.0184, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [35/126], Loss: 0.1675, Accuracy: 93.75%\n",
            "Epoch [3/15], Step [36/126], Loss: 0.0185, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [37/126], Loss: 0.0146, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [38/126], Loss: 0.0582, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [39/126], Loss: 0.0518, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [40/126], Loss: 0.0084, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [41/126], Loss: 0.0619, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [42/126], Loss: 0.0189, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [43/126], Loss: 0.0121, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [44/126], Loss: 0.0740, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [45/126], Loss: 0.0146, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [46/126], Loss: 0.0460, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [47/126], Loss: 0.0635, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [48/126], Loss: 0.0210, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [49/126], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [50/126], Loss: 0.0140, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [51/126], Loss: 0.0488, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [52/126], Loss: 0.0321, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [53/126], Loss: 0.0721, Accuracy: 93.75%\n",
            "Epoch [3/15], Step [54/126], Loss: 0.0678, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [55/126], Loss: 0.0609, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [56/126], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [57/126], Loss: 0.0037, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [58/126], Loss: 0.1446, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [59/126], Loss: 0.0360, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [60/126], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [61/126], Loss: 0.0456, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [62/126], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [63/126], Loss: 0.0228, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [64/126], Loss: 0.0346, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [65/126], Loss: 0.0254, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [66/126], Loss: 0.0234, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [67/126], Loss: 0.0142, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [68/126], Loss: 0.0238, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [69/126], Loss: 0.0114, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [70/126], Loss: 0.0220, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [71/126], Loss: 0.0591, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [72/126], Loss: 0.0110, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [73/126], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [74/126], Loss: 0.0114, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [75/126], Loss: 0.0352, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [76/126], Loss: 0.0134, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [77/126], Loss: 0.0250, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [78/126], Loss: 0.0166, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [79/126], Loss: 0.0156, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [80/126], Loss: 0.0111, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [81/126], Loss: 0.1230, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [82/126], Loss: 0.0177, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [83/126], Loss: 0.0083, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [84/126], Loss: 0.0153, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [85/126], Loss: 0.0305, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [86/126], Loss: 0.0218, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [87/126], Loss: 0.0459, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [88/126], Loss: 0.0152, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [89/126], Loss: 0.0889, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [90/126], Loss: 0.0239, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [91/126], Loss: 0.0133, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [92/126], Loss: 0.1398, Accuracy: 95.31%\n",
            "Epoch [3/15], Step [93/126], Loss: 0.0659, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [94/126], Loss: 0.0082, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [95/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [96/126], Loss: 0.0817, Accuracy: 93.75%\n",
            "Epoch [3/15], Step [97/126], Loss: 0.0301, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [98/126], Loss: 0.0135, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [99/126], Loss: 0.0815, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [100/126], Loss: 0.0669, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [101/126], Loss: 0.0229, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [102/126], Loss: 0.0665, Accuracy: 95.31%\n",
            "Epoch [3/15], Step [103/126], Loss: 0.0088, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [104/126], Loss: 0.0186, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [105/126], Loss: 0.1324, Accuracy: 95.31%\n",
            "Epoch [3/15], Step [106/126], Loss: 0.1247, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [107/126], Loss: 0.0877, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [108/126], Loss: 0.0317, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [109/126], Loss: 0.1602, Accuracy: 93.75%\n",
            "Epoch [3/15], Step [110/126], Loss: 0.0546, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [111/126], Loss: 0.0228, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [112/126], Loss: 0.0098, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [113/126], Loss: 0.0583, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [114/126], Loss: 0.0119, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [115/126], Loss: 0.0460, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [116/126], Loss: 0.1385, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [117/126], Loss: 0.0649, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [118/126], Loss: 0.0893, Accuracy: 96.88%\n",
            "Epoch [3/15], Step [119/126], Loss: 0.0440, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [120/126], Loss: 0.0076, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [121/126], Loss: 0.0320, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [122/126], Loss: 0.0556, Accuracy: 98.44%\n",
            "Epoch [3/15], Step [123/126], Loss: 0.0115, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [124/126], Loss: 0.0068, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [125/126], Loss: 0.0090, Accuracy: 100.00%\n",
            "Epoch [3/15], Step [126/126], Loss: 0.0192, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [1/126], Loss: 0.0119, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [2/126], Loss: 0.0199, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [3/126], Loss: 0.0112, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [4/126], Loss: 0.0079, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [5/126], Loss: 0.0147, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [6/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [7/126], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [8/126], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [9/126], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [10/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [11/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [12/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [13/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [14/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [15/126], Loss: 0.0093, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [16/126], Loss: 0.0063, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [17/126], Loss: 0.0209, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [18/126], Loss: 0.0280, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [19/126], Loss: 0.0900, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [20/126], Loss: 0.0181, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [21/126], Loss: 0.0277, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [22/126], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [23/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [24/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [25/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [26/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [27/126], Loss: 0.0398, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [28/126], Loss: 0.0725, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [29/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [30/126], Loss: 0.0195, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [31/126], Loss: 0.0141, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [32/126], Loss: 0.0172, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [33/126], Loss: 0.0116, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [34/126], Loss: 0.0149, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [35/126], Loss: 0.0054, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [36/126], Loss: 0.0079, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [37/126], Loss: 0.1304, Accuracy: 95.31%\n",
            "Epoch [4/15], Step [38/126], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [39/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [40/126], Loss: 0.0151, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [41/126], Loss: 0.0125, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [42/126], Loss: 0.1717, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [43/126], Loss: 0.0311, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [44/126], Loss: 0.0097, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [45/126], Loss: 0.0055, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [46/126], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [47/126], Loss: 0.0267, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [48/126], Loss: 0.0068, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [49/126], Loss: 0.0266, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [50/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [51/126], Loss: 0.0169, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [52/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [53/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [54/126], Loss: 0.0070, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [55/126], Loss: 0.0669, Accuracy: 95.31%\n",
            "Epoch [4/15], Step [56/126], Loss: 0.0146, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [57/126], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [58/126], Loss: 0.0058, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [59/126], Loss: 0.0583, Accuracy: 95.31%\n",
            "Epoch [4/15], Step [60/126], Loss: 0.0359, Accuracy: 96.88%\n",
            "Epoch [4/15], Step [61/126], Loss: 0.0177, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [62/126], Loss: 0.0056, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [63/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [64/126], Loss: 0.0097, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [65/126], Loss: 0.0498, Accuracy: 96.88%\n",
            "Epoch [4/15], Step [66/126], Loss: 0.0133, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [67/126], Loss: 0.2263, Accuracy: 93.75%\n",
            "Epoch [4/15], Step [68/126], Loss: 0.0665, Accuracy: 96.88%\n",
            "Epoch [4/15], Step [69/126], Loss: 0.0671, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [70/126], Loss: 0.0104, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [71/126], Loss: 0.0221, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [72/126], Loss: 0.0822, Accuracy: 96.88%\n",
            "Epoch [4/15], Step [73/126], Loss: 0.1955, Accuracy: 92.19%\n",
            "Epoch [4/15], Step [74/126], Loss: 0.0371, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [75/126], Loss: 0.0188, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [76/126], Loss: 0.0261, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [77/126], Loss: 0.0113, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [78/126], Loss: 0.0492, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [79/126], Loss: 0.1851, Accuracy: 90.62%\n",
            "Epoch [4/15], Step [80/126], Loss: 0.0238, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [81/126], Loss: 0.0343, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [82/126], Loss: 0.0154, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [83/126], Loss: 0.0162, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [84/126], Loss: 0.0492, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [85/126], Loss: 0.0557, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [86/126], Loss: 0.0335, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [87/126], Loss: 0.0172, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [88/126], Loss: 0.0160, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [89/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [90/126], Loss: 0.0165, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [91/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [92/126], Loss: 0.0417, Accuracy: 96.88%\n",
            "Epoch [4/15], Step [93/126], Loss: 0.0152, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [94/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [95/126], Loss: 0.0696, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [96/126], Loss: 0.0269, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [97/126], Loss: 0.0289, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [98/126], Loss: 0.0063, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [99/126], Loss: 0.0358, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [100/126], Loss: 0.0820, Accuracy: 96.88%\n",
            "Epoch [4/15], Step [101/126], Loss: 0.0204, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [102/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [103/126], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [104/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [105/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [106/126], Loss: 0.0137, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [107/126], Loss: 0.0965, Accuracy: 96.88%\n",
            "Epoch [4/15], Step [108/126], Loss: 0.0218, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [109/126], Loss: 0.0080, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [110/126], Loss: 0.0231, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [111/126], Loss: 0.0037, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [112/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [113/126], Loss: 0.0260, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [114/126], Loss: 0.0211, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [115/126], Loss: 0.0094, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [116/126], Loss: 0.0150, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [117/126], Loss: 0.0250, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [118/126], Loss: 0.0169, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [119/126], Loss: 0.0320, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [120/126], Loss: 0.0249, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [121/126], Loss: 0.0104, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [122/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [123/126], Loss: 0.0418, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [124/126], Loss: 0.0173, Accuracy: 98.44%\n",
            "Epoch [4/15], Step [125/126], Loss: 0.0099, Accuracy: 100.00%\n",
            "Epoch [4/15], Step [126/126], Loss: 0.0106, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [1/126], Loss: 0.0200, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [2/126], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [3/126], Loss: 0.0271, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [4/126], Loss: 0.0092, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [5/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [6/126], Loss: 0.0210, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [7/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [8/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [9/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [10/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [11/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [12/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [13/126], Loss: 0.0892, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [14/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [15/126], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [16/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [17/126], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [18/126], Loss: 0.0151, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [19/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [20/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [21/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [22/126], Loss: 0.0129, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [23/126], Loss: 0.0079, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [24/126], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [25/126], Loss: 0.0179, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [26/126], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [27/126], Loss: 0.0120, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [28/126], Loss: 0.0161, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [29/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [30/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [31/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [32/126], Loss: 0.0135, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [33/126], Loss: 0.0507, Accuracy: 96.88%\n",
            "Epoch [5/15], Step [34/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [35/126], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [36/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [37/126], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [38/126], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [39/126], Loss: 0.0166, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [40/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [41/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [42/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [43/126], Loss: 0.0225, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [44/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [45/126], Loss: 0.0104, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [46/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [47/126], Loss: 0.0128, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [48/126], Loss: 0.0860, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [49/126], Loss: 0.0162, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [50/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [51/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [52/126], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [53/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [54/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [55/126], Loss: 0.0108, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [56/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [57/126], Loss: 0.0346, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [58/126], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [59/126], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [60/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [61/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [62/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [63/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [64/126], Loss: 0.0192, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [65/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [66/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [67/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [68/126], Loss: 0.0190, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [69/126], Loss: 0.0083, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [70/126], Loss: 0.0289, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [71/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [72/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [73/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [74/126], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [75/126], Loss: 0.0321, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [76/126], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [77/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [78/126], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [79/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [80/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [81/126], Loss: 0.0056, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [82/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [83/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [84/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [85/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [86/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [87/126], Loss: 0.0106, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [88/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [89/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [90/126], Loss: 0.0121, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [91/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [92/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [93/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [94/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [95/126], Loss: 0.0311, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [96/126], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [97/126], Loss: 0.0143, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [98/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [99/126], Loss: 0.0948, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [100/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [101/126], Loss: 0.0079, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [102/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [103/126], Loss: 0.0957, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [104/126], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [105/126], Loss: 0.0238, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [106/126], Loss: 0.0412, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [107/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [108/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [109/126], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [110/126], Loss: 0.0336, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [111/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [112/126], Loss: 0.0342, Accuracy: 96.88%\n",
            "Epoch [5/15], Step [113/126], Loss: 0.0417, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [114/126], Loss: 0.0113, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [115/126], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [116/126], Loss: 0.0107, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [117/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [118/126], Loss: 0.0058, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [119/126], Loss: 0.0260, Accuracy: 98.44%\n",
            "Epoch [5/15], Step [120/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [121/126], Loss: 0.0058, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [122/126], Loss: 0.0100, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [123/126], Loss: 0.0070, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [124/126], Loss: 0.0108, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [125/126], Loss: 0.0166, Accuracy: 100.00%\n",
            "Epoch [5/15], Step [126/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [1/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [2/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [3/126], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [4/126], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [5/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [6/126], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [7/126], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [8/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [9/126], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [10/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [11/126], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [12/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [13/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [14/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [15/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [16/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [17/126], Loss: 0.0088, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [18/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [19/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [20/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [21/126], Loss: 0.0094, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [22/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [23/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [24/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [25/126], Loss: 0.0525, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [26/126], Loss: 0.0209, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [27/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [28/126], Loss: 0.0595, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [29/126], Loss: 0.0058, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [30/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [31/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [32/126], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [33/126], Loss: 0.0568, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [34/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [35/126], Loss: 0.0106, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [36/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [37/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [38/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [39/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [40/126], Loss: 0.0147, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [41/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [42/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [43/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [44/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [45/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [46/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [47/126], Loss: 0.0105, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [48/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [49/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [50/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [51/126], Loss: 0.0203, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [52/126], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [53/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [54/126], Loss: 0.0135, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [55/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [56/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [57/126], Loss: 0.0503, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [58/126], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [59/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [60/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [61/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [62/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [63/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [64/126], Loss: 0.0089, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [65/126], Loss: 0.0986, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [66/126], Loss: 0.0284, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [67/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [68/126], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [69/126], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [70/126], Loss: 0.0400, Accuracy: 96.88%\n",
            "Epoch [6/15], Step [71/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [72/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [73/126], Loss: 0.0233, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [74/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [75/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [76/126], Loss: 0.0762, Accuracy: 96.88%\n",
            "Epoch [6/15], Step [77/126], Loss: 0.0103, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [78/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [79/126], Loss: 0.0260, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [80/126], Loss: 0.0112, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [81/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [82/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [83/126], Loss: 0.0492, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [84/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [85/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [86/126], Loss: 0.0056, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [87/126], Loss: 0.0389, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [88/126], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [89/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [90/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [91/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [92/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [93/126], Loss: 0.0102, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [94/126], Loss: 0.0080, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [95/126], Loss: 0.1022, Accuracy: 96.88%\n",
            "Epoch [6/15], Step [96/126], Loss: 0.0216, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [97/126], Loss: 0.0039, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [98/126], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [99/126], Loss: 0.0212, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [100/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [101/126], Loss: 0.0129, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [102/126], Loss: 0.0121, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [103/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [104/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [105/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [106/126], Loss: 0.0135, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [107/126], Loss: 0.1268, Accuracy: 95.31%\n",
            "Epoch [6/15], Step [108/126], Loss: 0.0634, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [109/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [110/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [111/126], Loss: 0.0248, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [112/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [113/126], Loss: 0.0576, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [114/126], Loss: 0.0148, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [115/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [116/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [117/126], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [118/126], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [119/126], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [120/126], Loss: 0.0068, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [121/126], Loss: 0.0080, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [122/126], Loss: 0.0102, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [123/126], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [124/126], Loss: 0.0209, Accuracy: 98.44%\n",
            "Epoch [6/15], Step [125/126], Loss: 0.0107, Accuracy: 100.00%\n",
            "Epoch [6/15], Step [126/126], Loss: 0.0078, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [1/126], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [2/126], Loss: 0.0278, Accuracy: 96.88%\n",
            "Epoch [7/15], Step [3/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [4/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [5/126], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [6/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [7/126], Loss: 0.0292, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [8/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [9/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [10/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [11/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [12/126], Loss: 0.0137, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [13/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [14/126], Loss: 0.0273, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [15/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [16/126], Loss: 0.0123, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [17/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [18/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [19/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [20/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [21/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [22/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [23/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [24/126], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [25/126], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [26/126], Loss: 0.0230, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [27/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [28/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [29/126], Loss: 0.0289, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [30/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [31/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [32/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [33/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [34/126], Loss: 0.0100, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [35/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [36/126], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [37/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [38/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [39/126], Loss: 0.0234, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [40/126], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [41/126], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [42/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [43/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [44/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [45/126], Loss: 0.0298, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [46/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [47/126], Loss: 0.0134, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [48/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [49/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [50/126], Loss: 0.1004, Accuracy: 96.88%\n",
            "Epoch [7/15], Step [51/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [52/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [53/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [54/126], Loss: 0.0118, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [55/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [56/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [57/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [58/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [59/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [60/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [61/126], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [62/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [63/126], Loss: 0.0467, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [64/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [65/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [66/126], Loss: 0.0300, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [67/126], Loss: 0.0282, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [68/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [69/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [70/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [71/126], Loss: 0.0649, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [72/126], Loss: 0.0175, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [73/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [74/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [75/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [76/126], Loss: 0.0113, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [77/126], Loss: 0.0271, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [78/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [79/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [80/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [81/126], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [82/126], Loss: 0.0213, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [83/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [84/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [85/126], Loss: 0.0069, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [86/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [87/126], Loss: 0.0071, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [88/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [89/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [90/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [91/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [92/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [93/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [94/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [95/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [96/126], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [97/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [98/126], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [99/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [100/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [101/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [102/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [103/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [104/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [105/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [106/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [107/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [108/126], Loss: 0.0528, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [109/126], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [110/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [111/126], Loss: 0.0083, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [112/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [113/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [114/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [115/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [116/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [117/126], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [118/126], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [119/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [120/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [121/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [122/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [123/126], Loss: 0.0255, Accuracy: 98.44%\n",
            "Epoch [7/15], Step [124/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [125/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [7/15], Step [126/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [1/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [2/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [3/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [4/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [5/126], Loss: 0.0476, Accuracy: 98.44%\n",
            "Epoch [8/15], Step [6/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [7/126], Loss: 0.0099, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [8/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [9/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [10/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [11/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [12/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [13/126], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [14/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [15/126], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [16/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [17/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [18/126], Loss: 0.0206, Accuracy: 98.44%\n",
            "Epoch [8/15], Step [19/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [20/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [21/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [22/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [23/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [24/126], Loss: 0.1169, Accuracy: 98.44%\n",
            "Epoch [8/15], Step [25/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [26/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [27/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [28/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [29/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [30/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [31/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [32/126], Loss: 0.0054, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [33/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [34/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [35/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [36/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [37/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [38/126], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [39/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [40/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [41/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [42/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [43/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [44/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [45/126], Loss: 0.0056, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [46/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [47/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [48/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [49/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [50/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [51/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [52/126], Loss: 0.0061, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [53/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [54/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [55/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [56/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [57/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [58/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [59/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [60/126], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [61/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [62/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [63/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [64/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [65/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [66/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [67/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [68/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [69/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [70/126], Loss: 0.0736, Accuracy: 96.88%\n",
            "Epoch [8/15], Step [71/126], Loss: 0.0105, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [72/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [73/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [74/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [75/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [76/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [77/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [78/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [79/126], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [80/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [81/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [82/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [83/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [84/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [85/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [86/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [87/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [88/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [89/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [90/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [91/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [92/126], Loss: 0.0108, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [93/126], Loss: 0.0729, Accuracy: 98.44%\n",
            "Epoch [8/15], Step [94/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [95/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [96/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [97/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [98/126], Loss: 0.0273, Accuracy: 98.44%\n",
            "Epoch [8/15], Step [99/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [100/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [101/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [102/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [103/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [104/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [105/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [106/126], Loss: 0.0681, Accuracy: 98.44%\n",
            "Epoch [8/15], Step [107/126], Loss: 0.0417, Accuracy: 98.44%\n",
            "Epoch [8/15], Step [108/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [109/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [110/126], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [111/126], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [112/126], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [113/126], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [114/126], Loss: 0.1486, Accuracy: 96.88%\n",
            "Epoch [8/15], Step [115/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [116/126], Loss: 0.1039, Accuracy: 96.88%\n",
            "Epoch [8/15], Step [117/126], Loss: 0.0192, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [118/126], Loss: 0.0283, Accuracy: 98.44%\n",
            "Epoch [8/15], Step [119/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [120/126], Loss: 0.0490, Accuracy: 95.31%\n",
            "Epoch [8/15], Step [121/126], Loss: 0.0438, Accuracy: 96.88%\n",
            "Epoch [8/15], Step [122/126], Loss: 0.0437, Accuracy: 96.88%\n",
            "Epoch [8/15], Step [123/126], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [124/126], Loss: 0.0082, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [125/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [8/15], Step [126/126], Loss: 0.0039, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [1/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [2/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [3/126], Loss: 0.0257, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [4/126], Loss: 0.0257, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [5/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [6/126], Loss: 0.0081, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [7/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [8/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [9/126], Loss: 0.0299, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [10/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [11/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [12/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [13/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [14/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [15/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [16/126], Loss: 0.0246, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [17/126], Loss: 0.0348, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [18/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [19/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [20/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [21/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [22/126], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [23/126], Loss: 0.0117, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [24/126], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [25/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [26/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [27/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [28/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [29/126], Loss: 0.0094, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [30/126], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [31/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [32/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [33/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [34/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [35/126], Loss: 0.0277, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [36/126], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [37/126], Loss: 0.0092, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [38/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [39/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [40/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [41/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [42/126], Loss: 0.0719, Accuracy: 95.31%\n",
            "Epoch [9/15], Step [43/126], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [44/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [45/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [46/126], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [47/126], Loss: 0.0071, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [48/126], Loss: 0.2075, Accuracy: 92.19%\n",
            "Epoch [9/15], Step [49/126], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [50/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [51/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [52/126], Loss: 0.0436, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [53/126], Loss: 0.1906, Accuracy: 93.75%\n",
            "Epoch [9/15], Step [54/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [55/126], Loss: 0.1163, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [56/126], Loss: 0.0187, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [57/126], Loss: 0.0661, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [58/126], Loss: 0.0068, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [59/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [60/126], Loss: 0.0803, Accuracy: 96.88%\n",
            "Epoch [9/15], Step [61/126], Loss: 0.0803, Accuracy: 96.88%\n",
            "Epoch [9/15], Step [62/126], Loss: 0.0168, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [63/126], Loss: 0.0409, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [64/126], Loss: 0.1407, Accuracy: 96.88%\n",
            "Epoch [9/15], Step [65/126], Loss: 0.1763, Accuracy: 93.75%\n",
            "Epoch [9/15], Step [66/126], Loss: 0.0081, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [67/126], Loss: 0.0111, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [68/126], Loss: 0.0125, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [69/126], Loss: 0.0269, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [70/126], Loss: 0.0162, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [71/126], Loss: 0.1103, Accuracy: 96.88%\n",
            "Epoch [9/15], Step [72/126], Loss: 0.1432, Accuracy: 95.31%\n",
            "Epoch [9/15], Step [73/126], Loss: 0.0070, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [74/126], Loss: 0.0133, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [75/126], Loss: 0.0304, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [76/126], Loss: 0.0418, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [77/126], Loss: 0.0327, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [78/126], Loss: 0.0255, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [79/126], Loss: 0.0070, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [80/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [81/126], Loss: 0.0081, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [82/126], Loss: 0.0396, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [83/126], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [84/126], Loss: 0.0197, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [85/126], Loss: 0.0463, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [86/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [87/126], Loss: 0.0250, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [88/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [89/126], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [90/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [91/126], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [92/126], Loss: 0.0394, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [93/126], Loss: 0.0106, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [94/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [95/126], Loss: 0.0131, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [96/126], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [97/126], Loss: 0.0099, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [98/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [99/126], Loss: 0.0427, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [100/126], Loss: 0.0764, Accuracy: 96.88%\n",
            "Epoch [9/15], Step [101/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [102/126], Loss: 0.0795, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [103/126], Loss: 0.0037, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [104/126], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [105/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [106/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [107/126], Loss: 0.0193, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [108/126], Loss: 0.0659, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [109/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [110/126], Loss: 0.0290, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [111/126], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [112/126], Loss: 0.0469, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [113/126], Loss: 0.0170, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [114/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [115/126], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [116/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [117/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [118/126], Loss: 0.0061, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [119/126], Loss: 0.0097, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [120/126], Loss: 0.0340, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [121/126], Loss: 0.0143, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [122/126], Loss: 0.0186, Accuracy: 98.44%\n",
            "Epoch [9/15], Step [123/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [124/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [125/126], Loss: 0.0107, Accuracy: 100.00%\n",
            "Epoch [9/15], Step [126/126], Loss: 0.0085, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [1/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [2/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [3/126], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [4/126], Loss: 0.0128, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [5/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [6/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [7/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [8/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [9/126], Loss: 0.0228, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [10/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [11/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [12/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [13/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [14/126], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [15/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [16/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [17/126], Loss: 0.0094, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [18/126], Loss: 0.0089, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [19/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [20/126], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [21/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [22/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [23/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [24/126], Loss: 0.0288, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [25/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [26/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [27/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [28/126], Loss: 0.0194, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [29/126], Loss: 0.0088, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [30/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [31/126], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [32/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [33/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [34/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [35/126], Loss: 0.0339, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [36/126], Loss: 0.0170, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [37/126], Loss: 0.0284, Accuracy: 96.88%\n",
            "Epoch [10/15], Step [38/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [39/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [40/126], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [41/126], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [42/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [43/126], Loss: 0.0075, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [44/126], Loss: 0.0076, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [45/126], Loss: 0.0829, Accuracy: 96.88%\n",
            "Epoch [10/15], Step [46/126], Loss: 0.0061, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [47/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [48/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [49/126], Loss: 0.0361, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [50/126], Loss: 0.0949, Accuracy: 96.88%\n",
            "Epoch [10/15], Step [51/126], Loss: 0.0080, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [52/126], Loss: 0.0067, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [53/126], Loss: 0.0302, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [54/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [55/126], Loss: 0.0076, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [56/126], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [57/126], Loss: 0.0836, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [58/126], Loss: 0.1355, Accuracy: 96.88%\n",
            "Epoch [10/15], Step [59/126], Loss: 0.0759, Accuracy: 96.88%\n",
            "Epoch [10/15], Step [60/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [61/126], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [62/126], Loss: 0.0392, Accuracy: 96.88%\n",
            "Epoch [10/15], Step [63/126], Loss: 0.0282, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [64/126], Loss: 0.0547, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [65/126], Loss: 0.0110, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [66/126], Loss: 0.0118, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [67/126], Loss: 0.0117, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [68/126], Loss: 0.0082, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [69/126], Loss: 0.0102, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [70/126], Loss: 0.0107, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [71/126], Loss: 0.0987, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [72/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [73/126], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [74/126], Loss: 0.0683, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [75/126], Loss: 0.0504, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [76/126], Loss: 0.0204, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [77/126], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [78/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [79/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [80/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [81/126], Loss: 0.0671, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [82/126], Loss: 0.0460, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [83/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [84/126], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [85/126], Loss: 0.0165, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [86/126], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [87/126], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [88/126], Loss: 0.0375, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [89/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [90/126], Loss: 0.0109, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [91/126], Loss: 0.0341, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [92/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [93/126], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [94/126], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [95/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [96/126], Loss: 0.0137, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [97/126], Loss: 0.0039, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [98/126], Loss: 0.0121, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [99/126], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [100/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [101/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [102/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [103/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [104/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [105/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [106/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [107/126], Loss: 0.0236, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [108/126], Loss: 0.0045, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [109/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [110/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [111/126], Loss: 0.0061, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [112/126], Loss: 0.0491, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [113/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [114/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [115/126], Loss: 0.0143, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [116/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [117/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [118/126], Loss: 0.0169, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [119/126], Loss: 0.0350, Accuracy: 96.88%\n",
            "Epoch [10/15], Step [120/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [121/126], Loss: 0.0087, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [122/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [123/126], Loss: 0.0069, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [124/126], Loss: 0.0214, Accuracy: 98.44%\n",
            "Epoch [10/15], Step [125/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [10/15], Step [126/126], Loss: 0.0045, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [1/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [2/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [3/126], Loss: 0.0507, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [4/126], Loss: 0.0836, Accuracy: 96.88%\n",
            "Epoch [11/15], Step [5/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [6/126], Loss: 0.0668, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [7/126], Loss: 0.0092, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [8/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [9/126], Loss: 0.0798, Accuracy: 96.88%\n",
            "Epoch [11/15], Step [10/126], Loss: 0.0045, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [11/126], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [12/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [13/126], Loss: 0.0098, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [14/126], Loss: 0.0436, Accuracy: 96.88%\n",
            "Epoch [11/15], Step [15/126], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [16/126], Loss: 0.0121, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [17/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [18/126], Loss: 0.0146, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [19/126], Loss: 0.0144, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [20/126], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [21/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [22/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [23/126], Loss: 0.0168, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [24/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [25/126], Loss: 0.0098, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [26/126], Loss: 0.0291, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [27/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [28/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [29/126], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [30/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [31/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [32/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [33/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [34/126], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [35/126], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [36/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [37/126], Loss: 0.0614, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [38/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [39/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [40/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [41/126], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [42/126], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [43/126], Loss: 0.0682, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [44/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [45/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [46/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [47/126], Loss: 0.0688, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [48/126], Loss: 0.0469, Accuracy: 96.88%\n",
            "Epoch [11/15], Step [49/126], Loss: 0.0308, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [50/126], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [51/126], Loss: 0.0633, Accuracy: 95.31%\n",
            "Epoch [11/15], Step [52/126], Loss: 0.0130, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [53/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [54/126], Loss: 0.0223, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [55/126], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [56/126], Loss: 0.0409, Accuracy: 96.88%\n",
            "Epoch [11/15], Step [57/126], Loss: 0.0132, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [58/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [59/126], Loss: 0.0687, Accuracy: 96.88%\n",
            "Epoch [11/15], Step [60/126], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [61/126], Loss: 0.0225, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [62/126], Loss: 0.0097, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [63/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [64/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [65/126], Loss: 0.0960, Accuracy: 95.31%\n",
            "Epoch [11/15], Step [66/126], Loss: 0.0544, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [67/126], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [68/126], Loss: 0.0211, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [69/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [70/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [71/126], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [72/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [73/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [74/126], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [75/126], Loss: 0.0082, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [76/126], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [77/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [78/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [79/126], Loss: 0.0678, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [80/126], Loss: 0.0339, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [81/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [82/126], Loss: 0.0099, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [83/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [84/126], Loss: 0.0134, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [85/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [86/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [87/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [88/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [89/126], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [90/126], Loss: 0.0409, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [91/126], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [92/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [93/126], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [94/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [95/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [96/126], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [97/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [98/126], Loss: 0.0634, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [99/126], Loss: 0.0573, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [100/126], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [101/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [102/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [103/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [104/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [105/126], Loss: 0.0501, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [106/126], Loss: 0.1199, Accuracy: 96.88%\n",
            "Epoch [11/15], Step [107/126], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [108/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [109/126], Loss: 0.0095, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [110/126], Loss: 0.0067, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [111/126], Loss: 0.0106, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [112/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [113/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [114/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [115/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [116/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [117/126], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [118/126], Loss: 0.0148, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [119/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [120/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [121/126], Loss: 0.0449, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [122/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [123/126], Loss: 0.0089, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [124/126], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [11/15], Step [125/126], Loss: 0.0384, Accuracy: 98.44%\n",
            "Epoch [11/15], Step [126/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [1/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [2/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [3/126], Loss: 0.0149, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [4/126], Loss: 0.0054, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [5/126], Loss: 0.0134, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [6/126], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [7/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [8/126], Loss: 0.0055, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [9/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [10/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [11/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [12/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [13/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [14/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [15/126], Loss: 0.0136, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [16/126], Loss: 0.0142, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [17/126], Loss: 0.0342, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [18/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [19/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [20/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [21/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [22/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [23/126], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [24/126], Loss: 0.0055, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [25/126], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [26/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [27/126], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [28/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [29/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [30/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [31/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [32/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [33/126], Loss: 0.0105, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [34/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [35/126], Loss: 0.0284, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [36/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [37/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [38/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [39/126], Loss: 0.0133, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [40/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [41/126], Loss: 0.1083, Accuracy: 96.88%\n",
            "Epoch [12/15], Step [42/126], Loss: 0.0166, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [43/126], Loss: 0.0234, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [44/126], Loss: 0.0136, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [45/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [46/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [47/126], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [48/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [49/126], Loss: 0.0230, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [50/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [51/126], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [52/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [53/126], Loss: 0.0082, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [54/126], Loss: 0.0642, Accuracy: 96.88%\n",
            "Epoch [12/15], Step [55/126], Loss: 0.0478, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [56/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [57/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [58/126], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [59/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [60/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [61/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [62/126], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [63/126], Loss: 0.0086, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [64/126], Loss: 0.0098, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [65/126], Loss: 0.0037, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [66/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [67/126], Loss: 0.0285, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [68/126], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [69/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [70/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [71/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [72/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [73/126], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [74/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [75/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [76/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [77/126], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [78/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [79/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [80/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [81/126], Loss: 0.0941, Accuracy: 96.88%\n",
            "Epoch [12/15], Step [82/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [83/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [84/126], Loss: 0.0105, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [85/126], Loss: 0.0068, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [86/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [87/126], Loss: 0.0269, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [88/126], Loss: 0.0655, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [89/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [90/126], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [91/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [92/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [93/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [94/126], Loss: 0.0260, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [95/126], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [96/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [97/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [98/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [99/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [100/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [101/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [102/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [103/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [104/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [105/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [106/126], Loss: 0.0079, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [107/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [108/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [109/126], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [110/126], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [111/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [112/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [113/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [114/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [115/126], Loss: 0.0120, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [116/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [117/126], Loss: 0.0123, Accuracy: 98.44%\n",
            "Epoch [12/15], Step [118/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [119/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [120/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [121/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [122/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [123/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [124/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [125/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [12/15], Step [126/126], Loss: 0.0207, Accuracy: 98.33%\n",
            "Epoch [13/15], Step [1/126], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [2/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [3/126], Loss: 0.0243, Accuracy: 98.44%\n",
            "Epoch [13/15], Step [4/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [5/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [6/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [7/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [8/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [9/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [10/126], Loss: 0.0129, Accuracy: 98.44%\n",
            "Epoch [13/15], Step [11/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [12/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [13/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [14/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [15/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [16/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [17/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [18/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [19/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [20/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [21/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [22/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [23/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [24/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [25/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [26/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [27/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [28/126], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [29/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [30/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [31/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [32/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [33/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [34/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [35/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [36/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [37/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [38/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [39/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [40/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [41/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [42/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [43/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [44/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [45/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [46/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [47/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [48/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [49/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [50/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [51/126], Loss: 0.0070, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [52/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [53/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [54/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [55/126], Loss: 0.0101, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [56/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [57/126], Loss: 0.0070, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [58/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [59/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [60/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [61/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [62/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [63/126], Loss: 0.0226, Accuracy: 98.44%\n",
            "Epoch [13/15], Step [64/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [65/126], Loss: 0.0108, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [66/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [67/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [68/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [69/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [70/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [71/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [72/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [73/126], Loss: 0.0204, Accuracy: 98.44%\n",
            "Epoch [13/15], Step [74/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [75/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [76/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [77/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [78/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [79/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [80/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [81/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [82/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [83/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [84/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [85/126], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [86/126], Loss: 0.0310, Accuracy: 98.44%\n",
            "Epoch [13/15], Step [87/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [88/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [89/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [90/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [91/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [92/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [93/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [94/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [95/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [96/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [97/126], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [98/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [99/126], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [100/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [101/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [102/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [103/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [104/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [105/126], Loss: 0.0233, Accuracy: 98.44%\n",
            "Epoch [13/15], Step [106/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [107/126], Loss: 0.0615, Accuracy: 96.88%\n",
            "Epoch [13/15], Step [108/126], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [109/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [110/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [111/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [112/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [113/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [114/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [115/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [116/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [117/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [118/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [119/126], Loss: 0.0096, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [120/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [121/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [122/126], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [123/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [124/126], Loss: 0.0138, Accuracy: 98.44%\n",
            "Epoch [13/15], Step [125/126], Loss: 0.0045, Accuracy: 100.00%\n",
            "Epoch [13/15], Step [126/126], Loss: 0.0241, Accuracy: 98.33%\n",
            "Epoch [14/15], Step [1/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [2/126], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [3/126], Loss: 0.0194, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [4/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [5/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [6/126], Loss: 0.0097, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [7/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [8/126], Loss: 0.0424, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [9/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [10/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [11/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [12/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [13/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [14/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [15/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [16/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [17/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [18/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [19/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [20/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [21/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [22/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [23/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [24/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [25/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [26/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [27/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [28/126], Loss: 0.0309, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [29/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [30/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [31/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [32/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [33/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [34/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [35/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [36/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [37/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [38/126], Loss: 0.0128, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [39/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [40/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [41/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [42/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [43/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [44/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [45/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [46/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [47/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [48/126], Loss: 0.0274, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [49/126], Loss: 0.0161, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [50/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [51/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [52/126], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [53/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [54/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [55/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [56/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [57/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [58/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [59/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [60/126], Loss: 0.0095, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [61/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [62/126], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [63/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [64/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [65/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [66/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [67/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [68/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [69/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [70/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [71/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [72/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [73/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [74/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [75/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [76/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [77/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [78/126], Loss: 0.0092, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [79/126], Loss: 0.0083, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [80/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [81/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [82/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [83/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [84/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [85/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [86/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [87/126], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [88/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [89/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [90/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [91/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [92/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [93/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [94/126], Loss: 0.0199, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [95/126], Loss: 0.0655, Accuracy: 96.88%\n",
            "Epoch [14/15], Step [96/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [97/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [98/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [99/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [100/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [101/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [102/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [103/126], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [104/126], Loss: 0.0269, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [105/126], Loss: 0.0307, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [106/126], Loss: 0.0070, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [107/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [108/126], Loss: 0.0194, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [109/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [110/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [111/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [112/126], Loss: 0.0684, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [113/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [114/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [115/126], Loss: 0.0377, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [116/126], Loss: 0.1229, Accuracy: 96.88%\n",
            "Epoch [14/15], Step [117/126], Loss: 0.0136, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [118/126], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [119/126], Loss: 0.0047, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [120/126], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [121/126], Loss: 0.0277, Accuracy: 98.44%\n",
            "Epoch [14/15], Step [122/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [123/126], Loss: 0.0104, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [124/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [125/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [14/15], Step [126/126], Loss: 0.0300, Accuracy: 98.33%\n",
            "Epoch [15/15], Step [1/126], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [2/126], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [3/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [4/126], Loss: 0.0095, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [5/126], Loss: 0.0113, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [6/126], Loss: 0.0211, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [7/126], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [8/126], Loss: 0.0103, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [9/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [10/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [11/126], Loss: 0.0105, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [12/126], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [13/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [14/126], Loss: 0.0232, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [15/126], Loss: 0.0363, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [16/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [17/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [18/126], Loss: 0.0156, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [19/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [20/126], Loss: 0.0136, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [21/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [22/126], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [23/126], Loss: 0.0204, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [24/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [25/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [26/126], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [27/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [28/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [29/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [30/126], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [31/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [32/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [33/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [34/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [35/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [36/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [37/126], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [38/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [39/126], Loss: 0.0247, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [40/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [41/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [42/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [43/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [44/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [45/126], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [46/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [47/126], Loss: 0.0240, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [48/126], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [49/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [50/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [51/126], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [52/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [53/126], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [54/126], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [55/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [56/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [57/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [58/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [59/126], Loss: 0.0233, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [60/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [61/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [62/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [63/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [64/126], Loss: 0.0161, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [65/126], Loss: 0.0177, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [66/126], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [67/126], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [68/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [69/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [70/126], Loss: 0.0295, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [71/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [72/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [73/126], Loss: 0.0319, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [74/126], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [75/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [76/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [77/126], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [78/126], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [79/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [80/126], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [81/126], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [82/126], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [83/126], Loss: 0.0432, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [84/126], Loss: 0.0118, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [85/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [86/126], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [87/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [88/126], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [89/126], Loss: 0.0085, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [90/126], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [91/126], Loss: 0.0146, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [92/126], Loss: 0.0784, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [93/126], Loss: 0.0721, Accuracy: 96.88%\n",
            "Epoch [15/15], Step [94/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [95/126], Loss: 0.0117, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [96/126], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [97/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [98/126], Loss: 0.0328, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [99/126], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [100/126], Loss: 0.0056, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [101/126], Loss: 0.0738, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [102/126], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [103/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [104/126], Loss: 0.0092, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [105/126], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [106/126], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [107/126], Loss: 0.0242, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [108/126], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [109/126], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [110/126], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [111/126], Loss: 0.0208, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [112/126], Loss: 0.1913, Accuracy: 95.31%\n",
            "Epoch [15/15], Step [113/126], Loss: 0.0142, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [114/126], Loss: 0.0058, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [115/126], Loss: 0.0079, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [116/126], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [117/126], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [118/126], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [119/126], Loss: 0.0076, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [120/126], Loss: 0.0737, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [121/126], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [122/126], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [123/126], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [15/15], Step [124/126], Loss: 0.0151, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [125/126], Loss: 0.0741, Accuracy: 98.44%\n",
            "Epoch [15/15], Step [126/126], Loss: 0.0021, Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5211dfbd829f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-aa98b47df989>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-38113e21f0a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# load the saved model\n",
        "model = CustomVGG()\n",
        "model.load_state_dict(torch.load('hijab_model_torch.pth')) \n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHfV7rnscWSM",
        "outputId": "e58b5618-234d-4352-8413-c6e67e6fe28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomVGG(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
              "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=2304, out_features=500, bias=True)\n",
              "  (fc2): Linear(in_features=500, out_features=50, bias=True)\n",
              "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "Gb0oa9glr7l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        correct = 0\n",
        "        for images_batch , labels_batch in test_loader:\n",
        "            images_batch, labels_batch = images_batch.cuda(), labels_batch.cuda() \n",
        "            output=model(images_batch)\n",
        "            \n",
        "            _,predicted=torch.max(output.data,dim=-1)\n",
        "            \n",
        "            correct+= (predicted == labels_batch).sum().item()\n",
        "\n",
        "        accuracy=(correct/len(test_loader.dataset))*100\n",
        "        print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMibuzSOkqLU",
        "outputId": "d238b8db-93ed-4f5d-cd9f-c429782083d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.6003996003996%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform_normjj = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "classes = train_loader.dataset.classes #['hijab', 'nonhijab']\n",
        "def pred_glasses(img):\n",
        "         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "         # get normalized image\n",
        "         img_normalized = transform_normjj(img).float()\n",
        "         img_normalized = img_normalized.unsqueeze_(0)\n",
        "         img_normalized = img_normalized.to(device)\n",
        "         with torch.no_grad():\n",
        "            model.eval()  \n",
        "            output = model(img_normalized)\n",
        "            index  = output.data.cpu().numpy().argmax()\n",
        "            class_name = classes[index]\n",
        "            return class_name, index"
      ],
      "metadata": {
        "id": "dqm3bb34dsEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/324659816_1167662230544924_8200639109336242480_n.jpg')\n",
        "pred_glasses(img) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTw8NNElec0E",
        "outputId": "3ddeb377-d394-4f72-b377-e147003e9a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('nonhijab', 1)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://m.media-amazon.com/images/I/41+x3N0H14L._AC_UF1000,1000_QL80_.jpg"
      ],
      "metadata": {
        "id": "n9ZoxM7aoXeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('41+x3N0H14L._AC_UF1000,1000_QL80_.jpg')\n",
        "pred_glasses(img) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pms9ETHbn5P5",
        "outputId": "753c7a98-3aa1-4d90-ba9a-2668492b6458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hijab', 0)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}